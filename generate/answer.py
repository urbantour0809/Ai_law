from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# âœ… EXAONE ëª¨ë¸ ë¡œë“œ ì„¤ì •
MODEL_NAME = "LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct"
device = "cuda" if torch.cuda.is_available() else "cpu"

# âœ… CUDA ì˜¤ë¥˜ ì²´í¬
print(f"âœ… í˜„ì¬ PyTorch CUDA ë²„ì „: {torch.version.cuda}")
print(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€: {torch.cuda.is_available()}")
print(f"âœ… í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ GPU ê°œìˆ˜: {torch.cuda.device_count()}ê°œ")

if device == "cuda":
    try:
        torch.cuda.current_device()
        print("âœ… CUDA ë””ë°”ì´ìŠ¤ê°€ ì •ìƒì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ CUDA ë””ë°”ì´ìŠ¤ ë¬¸ì œ ë°œìƒ: {e}")
        device = "cpu"  # ì˜¤ë¥˜ ë°œìƒ ì‹œ CPUë¡œ ê°•ì œ ë³€ê²½

# âœ… ëª¨ë¸ ë¡œë“œ (VRAM ì ˆì•½ì„ ìœ„í•´ `torch_dtype=torch.float16` ì‚¬ìš©)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME, trust_remote_code=True, torch_dtype=torch.float16
).to(device)

def generate_answer(query, relevant_docs, sources, scores):
    """ âœ… EXAONE ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ ë²•ë¥  ë‹µë³€ ìƒì„± (GPU í™œìš©) """

    # âœ… ê²€ìƒ‰ëœ ë²•ë¥  ì •ë³´ê°€ ë¶€ì¡±í•  ê²½ìš° ê¸°ë³¸ ë©”ì‹œì§€ ì„¤ì •
    if not relevant_docs:
        relevant_docs = ["ğŸ“Œ ì°¸ê³ í•  ë²•ë¥  ì¡°í•­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ë²•ë¥  ì›ì¹™ì„ ì ìš©í•˜ì„¸ìš”."]

    # âœ… `scores`ê°€ ë¦¬ìŠ¤íŠ¸ ì•ˆì— ë¦¬ìŠ¤íŠ¸(`list[list]`)ì¸ì§€ í™•ì¸ í›„ ë³€í™˜
    if isinstance(scores[0], list):
        scores = [s for sublist in scores for s in sublist]

    # âœ… í”„ë¡¬í”„íŠ¸ ìµœì í™”
    prompt = f"""
    [ì‚¬ìš©ì ì§ˆë¬¸]
    ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë³€í˜¸ì‚¬ì…ë‹ˆë‹¤.
    ê·¸ë¦¬ê³  ëª¨ë“  ë‹µë³€ì€ í•œêµ­ì–´ë¡œ í•´ì£¼ì„¸ìš”.
    ê³ ê°ì˜ ì§ˆë¬¸ì„ ì„¸ë°€í•˜ê²Œ ë¶„ì„í•˜ì—¬, ì „ë¬¸ì ì¸ ë‹µë³€ì„ ì‚¬ìš©ìì—ê²Œ ì œê³µí•´ì£¼ì„¸ìš”.
    ì£¼ë¡œ ì•„ë˜ì— ê´€ë ¨ ë²•ë¥  ë° íŒë¡€ë¥¼ ìœ„ì£¼ë¡œ ë‹µë³€ì„ ìƒìƒí•˜ì„¸ìš”.
    {query}

    [ê´€ë ¨ ë²•ë¥  ë° íŒë¡€]
    {relevant_docs}

    [ë³€í˜¸ì‚¬ ë‹µë³€]
    """

    # âœ… tokenizer ì‹¤í–‰ ì „ì— `query` íƒ€ì… ì²´í¬
    if not isinstance(query, str) or not query.strip():
        raise ValueError("âŒ ì˜¤ë¥˜: `query` ê°’ì´ ì˜¬ë°”ë¥¸ ë¬¸ìì—´ì´ ì•„ë‹™ë‹ˆë‹¤.")

    # âœ… ëª¨ë¸ ì…ë ¥ ì²˜ë¦¬ (GPUë¡œ ì´ë™)
    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    # âœ… ëª¨ë¸ ì‹¤í–‰ (ë‹µë³€ ìƒì„±)
    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_length=4096,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            repetition_penalty=1.1,
            stopping_criteria=None,
            pad_token_id=tokenizer.eos_token_id
        )

    answer = tokenizer.decode(output[0], skip_special_tokens=True).split("[ë³€í˜¸ì‚¬ ë‹µë³€]")[-1].strip()

    return f""":{answer}"""
