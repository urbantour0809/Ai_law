import os
import chromadb
import torch
from transformers import AutoTokenizer, AutoModel
import numpy as np  # âœ… numpy ì¶”ê°€
import re

# âœ… ChromaDB ì„¤ì •
CHROMA_DB_PATH = "../dataset/chroma_db"
chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
legal_collection = chroma_client.get_or_create_collection(name="legal_data")

# âœ… Fine-tuned `legal-bert-base` ëª¨ë¸ ë¡œë“œ
MODEL_PATH = "../ft_legal_bert/checkpoint-1185"
device = "cuda" if torch.cuda.is_available() else "cpu"
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)

# âœ… VRAM ìµœì í™”ë¥¼ ìœ„í•´ `.half()` ì‚¬ìš©
embedding_model = AutoModel.from_pretrained(MODEL_PATH, torch_dtype=torch.float16).to(device)

def embed_text(text):
    """ âœ… Fine-tuned ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ì„ ë²¡í„°í™” (GPU í™œìš©) """

    # âœ… None ê°’ ì²´í¬
    if text is None:
        raise ValueError("âŒ ì˜¤ë¥˜: `text` ê°’ì´ None ì…ë‹ˆë‹¤. ì˜¬ë°”ë¥¸ ë¬¸ìì—´ì„ ì…ë ¥í•˜ì„¸ìš”.")

    # âœ… ë¬¸ìì—´ ë³€í™˜ ë° ê³µë°± ì •ë¦¬
    if isinstance(text, list):
        text = " ".join(map(str, text))  # ë¦¬ìŠ¤íŠ¸ â†’ ë¬¸ìì—´ ë³€í™˜

    if isinstance(text, (int, float, np.ndarray)):
        text = str(text)  # ìˆ«ìë‚˜ numpy ë°°ì—´ â†’ ë¬¸ìì—´ ë³€í™˜

    if not isinstance(text, str):
        raise ValueError(f"âŒ ì˜¤ë¥˜: `text`ì˜ íƒ€ì…ì´ ë¬¸ìì—´ì´ ì•„ë‹™ë‹ˆë‹¤. (í˜„ì¬ íƒ€ì…: {type(text)})")

    # âœ… ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±° ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°
    text = re.sub(r"\s+", " ", text.strip())  # ì—¬ëŸ¬ ê°œ ê³µë°± â†’ í•˜ë‚˜ë¡œ ë³€í™˜

    if not text:
        raise ValueError("âŒ ì˜¤ë¥˜: ë¹ˆ ë¬¸ìì—´ì´ ì…ë ¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # âœ… í† í¬ë‚˜ì´ì € ì‹¤í–‰ ì „ì— í…ŒìŠ¤íŠ¸
    try:
        _ = tokenizer.encode(text, add_special_tokens=True)
    except Exception as e:
        raise ValueError(f"âŒ í† í¬ë‚˜ì´ì € ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # âœ… í…ìŠ¤íŠ¸ ì„ë² ë”© ì²˜ë¦¬
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding="max_length", max_length=512).to(device)
    
    with torch.no_grad():
        outputs = embedding_model(**inputs)
        embedding = outputs.last_hidden_state[:, 0, :].squeeze(0)

    return embedding.cpu().numpy().flatten().tolist()  # âœ… 1ì°¨ì› ë¦¬ìŠ¤íŠ¸ ë³€í™˜

def get_relevant_docs(query, top_k=5):
    """ âœ… ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë²¡í„°í™”í•˜ê³ , ê´€ë ¨ ë²•ë¥  ë°ì´í„°ë¥¼ ê²€ìƒ‰ """

    # âœ… ì…ë ¥ê°’ ê²€ì¦
    if not isinstance(query, str):
        raise ValueError("âŒ ì˜¤ë¥˜: ì…ë ¥ëœ ì§ˆë¬¸ì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (ë¬¸ìì—´ì´ ì•„ë‹™ë‹ˆë‹¤.)")

    query = query.strip()  # ì•ë’¤ ê³µë°± ì œê±°
    query = re.sub(r"\s+", " ", query)  # ì—¬ëŸ¬ ê°œ ê³µë°± â†’ í•˜ë‚˜ë¡œ ë³€í™˜

    if not query:
        raise ValueError("âŒ ì˜¤ë¥˜: ë¹ˆ ë¬¸ìì—´ì´ ì…ë ¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    query_embedding = embed_text(query)

    # âœ… `query_embedding`ì´ ì˜¬ë°”ë¥¸ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
    if isinstance(query_embedding, np.ndarray):
        query_embedding = query_embedding.tolist()  # numpy ë°°ì—´ â†’ ë¦¬ìŠ¤íŠ¸ ë³€í™˜

    if isinstance(query_embedding[0], list):  # ğŸ”¥ ì¤‘ì²© ë¦¬ìŠ¤íŠ¸ ë°©ì§€
        query_embedding = query_embedding[0]

    # âœ… ChromaDBì—ì„œ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ë²•ë¥  ë°ì´í„° ê²€ìƒ‰
    results = legal_collection.query(query_embeddings=[query_embedding], n_results=top_k)

    relevant_texts = []
    sources = []
    scores = []
    law_numbers = []  # âœ… ë²•ë¥  ë²ˆí˜¸/íŒë¡€ ë²ˆí˜¸ ì¶”ê°€ ì €ì¥

    # âœ… ê²€ìƒ‰ëœ ê²°ê³¼ ê°€ê³µ
    for i, meta_list in enumerate(results.get("metadatas", [])):
        for meta in meta_list:
            text = meta.get("text", results["documents"][i] if results.get("documents") else None)
            score = results.get("distances", [])[i] if results.get("distances") else 0.0
            law_number = meta.get("law_number", meta.get("case_number", "ë²•ë¥  ë²ˆí˜¸ ì—†ìŒ"))  # âœ… ì‚¬ê±´ë²ˆí˜¸ or ë²•ë¥  ë²ˆí˜¸ ì¶”ê°€

            if isinstance(text, list):
                text = "\n".join([t if t is not None else "" for t in text])

            if text and text.strip():
                relevant_texts.append(text)
                sources.append(meta.get("source", "ì¶œì²˜ ì •ë³´ ì—†ìŒ"))
                scores.append(score)
                law_numbers.append(law_number)  # âœ… ë²•ë¥  ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€

    # âœ… 2ì°¨ì› ë¦¬ìŠ¤íŠ¸(`list[list[float]]`)ì¸ ê²½ìš° 1ì°¨ì› ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    scores = [s for sublist in scores for s in (sublist if isinstance(sublist, list) else [sublist])]

    return relevant_texts, sources, scores, law_numbers
