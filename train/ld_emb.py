import os
import json
import chromadb
import torch
from transformers import AutoModel, AutoTokenizer
from sklearn.feature_extraction.text import TfidfVectorizer

# âœ… ë°ì´í„° ê²½ë¡œ ì„¤ì •
JSON_PATH = os.path.abspath("../dataset/íŒë¡€ëª©ë¡.json")

# âœ… GPU ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"

# âœ… ChromaDB ì„¤ì •
CHROMA_DB_PATH = "../dataset/chroma_db"
chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
collection = chroma_client.get_or_create_collection(name="legal_data")

# âœ… Fine-tuned `legal-bert-base` ëª¨ë¸ ë¡œë“œ
MODEL_PATH = "../ft_legal_bert/checkpoint-1185"
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
embedding_model = AutoModel.from_pretrained(MODEL_PATH).to(device)

# âœ… ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸
STOPWORDS = set([
    "ìˆë‹¤", "í•˜ëŠ”", "ë˜ì–´", "ë•Œë¬¸", "ìœ„í•´", "ìœ¼ë¡œ", "ê·¸ë¦¬ê³ ", "ë”°ë¼ì„œ", "í•˜ì§€ë§Œ", "ëŒ€í•˜ì—¬", 
    "ê°™ì€", "ê·¸ëŸ¬í•œ", "ê²½ìš°", "ê·¸ëŸ°ë°", "ë¿ë§Œ", "ì´ëŸ¬í•œ", "ê·¸ê²ƒ", "ì´ê²ƒ", "ì €ê²ƒ",
    "ì´ë‹¤", "ì˜€ë‹¤", "ë˜ì§€", "ì—†ì´", "í•˜ëŠ”", "í•˜ë©°", "ë¼ê³ ", "ê¹Œì§€", "í•˜ë©´ì„œ", "ë“±ë“±",
    "ë°", "ì´ë‹¤", "ì—ì„œ", "ê³ ", "ê°€", "ì˜", "ë¥¼", "ì„", "ì—", "ëŠ”", "ë„", "ê³¼", "ì™€", "ì€"
])

def extract_keywords(text, top_n=5):
    """ âœ… TF-IDFë¥¼ í™œìš©í•˜ì—¬ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ (ë¶ˆìš©ì–´ ì œê±° í¬í•¨) """
    if not text.strip():  # âœ… ë¹ˆ ë¬¸ìì—´ ì²´í¬
        return "í‚¤ì›Œë“œ ì—†ìŒ"
    
    words = text.split()
    if len(words) < 3:  # âœ… ë‹¨ì–´ê°€ ë„ˆë¬´ ì ì„ ê²½ìš° TF-IDF ì ìš© ë¶ˆê°€
        return "í‚¤ì›Œë“œ ì—†ìŒ"

    vectorizer = TfidfVectorizer(stop_words=list(STOPWORDS), max_features=top_n)

    try:
        tfidf_matrix = vectorizer.fit_transform([text])
        feature_names = vectorizer.get_feature_names_out()
        return ", ".join(feature_names) if feature_names.size > 0 else "í‚¤ì›Œë“œ ì—†ìŒ"
    except ValueError:
        return "í‚¤ì›Œë“œ ì—†ìŒ"

def embed_text(text):
    """ âœ… Fine-tuned ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ì„ ë²¡í„°í™” (GPU í™œìš©) """
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding="max_length", max_length=512).to(device)
    with torch.no_grad():
        outputs = embedding_model(**inputs)
    return outputs.last_hidden_state[:, 0, :].cpu().numpy().tolist()[0]

# âœ… ì‹¤í–‰
if __name__ == "__main__":
    print("ğŸ“Œ íŒë¡€ ëª©ë¡ ë°ì´í„° ë²¡í„°í™” ì‹œì‘...")

    with open(JSON_PATH, "r", encoding="utf-8") as f:
        data_list = json.load(f)

    for data in data_list:
        case_number = data.get("ì‚¬ê±´ë²ˆí˜¸", "ì‚¬ê±´ë²ˆí˜¸ ì—†ìŒ").strip()
        title = data.get("ì œëª©", "").strip()
        ruling = data.get("íŒì‹œì‚¬í•­", "").strip()

        if not title and not ruling:  # âœ… ë‘˜ ë‹¤ ë¹„ì–´ ìˆìœ¼ë©´ ìŠ¤í‚µ
            print(f"âš  ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ê±´ë„ˆëœ€: {case_number}")
            continue

        text_data = f"{title} {ruling}".strip()

        keywords = extract_keywords(text_data)
        embedding = embed_text(text_data)

        # âœ… ì¤‘ë³µ í™•ì¸ í›„ ë°ì´í„° ì¶”ê°€
        existing_data = collection.get(ids=[case_number])
        if existing_data and existing_data["ids"]:
            print(f"ğŸ”„ ê¸°ì¡´ ë°ì´í„° ìŠ¤í‚µ: {case_number}")
            continue

        print(f"âœ… ìƒˆë¡œìš´ ë°ì´í„° ì¶”ê°€: {case_number}")
        collection.add(
            ids=[case_number],
            embeddings=[embedding],
            metadatas=[{"text": text_data, "keywords": keywords}]
        )

    print("âœ… ëª¨ë“  íŒë¡€ ëª©ë¡ ë°ì´í„° ë²¡í„°í™” ì™„ë£Œ!")
